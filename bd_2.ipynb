{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ignacioaranguren1/bd_2/blob/main/bd_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vF6fH-KFdDnq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import keras_tuner\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras_tuner.tuners import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "datapath = '/Users/ignacioaranguren/bd_2/data'\n",
    "os.chdir(datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUSZgH01c0gK"
   },
   "source": [
    "1.In the data used by Gu, Kelly and Xiu (RFS 2019 – provided in class), use a similar procedure to theirs to predict stock returns with neural networks. Start by finding a suitable baseline configuration, and use a validation procedure to pick optimal hyperparameters for three neural network models: One with 2 hidden layers, one with 3 hidden layers, and one with 4 hidden layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JN8g2yI6c0gN"
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('returns_chars_panel.pkl')\n",
    "macro = pd.read_pickle('macro_timeseries.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uUS00_JCrz2m"
   },
   "outputs": [],
   "source": [
    "def train_validation_test_split(data,train_end_date,validation_end_date):\n",
    "  tmp = data.reset_index()\n",
    "  train = tmp[tmp.date<=train_end_date].set_index(['date','permno'],drop=True)\n",
    "  validation = tmp[(tmp.date>train_end_date) & (tmp.date<=validation_end_date)].set_index(['date','permno'],drop=True)\n",
    "  test = tmp[tmp.date>validation_end_date].set_index(['date','permno'],drop=True)\n",
    "  return train,validation,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2ZXV1zHXnrbV"
   },
   "outputs": [],
   "source": [
    "data_merged = pd.merge(data,macro,on=['date'])\n",
    "datelist = list(set(data_merged['date']))\n",
    "datelist.sort()\n",
    "data_merged.set_index(['date','permno'],drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "QKYOGMqDoGqO"
   },
   "outputs": [],
   "source": [
    "train_ratio = 0.5\n",
    "validation_ratio = 0.25\n",
    "train_date = datelist[int(len(datelist)*train_ratio)]\n",
    "validation_date = datelist[int(len(datelist)*(train_ratio+validation_ratio))]\n",
    "X = data_merged.iloc[:,3:].copy()\n",
    "y = data_merged['excess_ret'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "JiHQP_Xmrv9s"
   },
   "outputs": [],
   "source": [
    "X_train,X_validation,X_test = train_validation_test_split(X,train_date,validation_date)\n",
    "y_train,y_validation,y_test = train_validation_test_split(y,train_date,validation_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, x, y, validation_data, **kwargs):\n",
    "  model.fit(x, y, **kwargs)\n",
    "  x_val, y_val = validation_data\n",
    "  y_pred = model.predict(x_val)\n",
    "  # Return a single float to minimize.\n",
    "  return np.mean((y_pred - y_val)**2)\n",
    "\n",
    "EPOCHS = 16\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "def keras_code(n_layers, units, dropout_rate, learning_rate):\n",
    "    # Build model\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(shape=(105,)))\n",
    "    for i in range(n_layers):\n",
    "        model.add(layers.Dense(units=units[i], activation='relu'))\n",
    "    model.add(layers.Dropout(rate=dropout_rate))\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mse')\n",
    "    return fit(\n",
    "        model, \n",
    "        X_train.values, \n",
    "        y_train.values,\n",
    "        validation_data=(X_validation.values, y_validation.values),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS\n",
    "        )\n",
    "    \n",
    "    \n",
    "class MyTuner(keras_tuner.RandomSearch): \n",
    "    def __init__(self, n_layers, *args, **kwargs):\n",
    "        self.n_layers = n_layers\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def run_trial(self, trial, **kwargs):\n",
    "        hp = trial.hyperparameters\n",
    "        return keras_code(\n",
    "            n_layers=self.n_layers,\n",
    "            units=[hp.Int(f'units_{i + 1}',min_value=16,max_value=160,step=16) for i in range(self.n_layers)],\n",
    "            dropout_rate=hp.Choice('dropout', values=[0.2,0.4,0.6,0.8]),\n",
    "            learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "        )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "4dpSUJZi1quh"
   },
   "outputs": [],
   "source": [
    "MAX_TRIALS = 10\n",
    "EXECUTION_PER_TRIAL = 2\n",
    "\n",
    "def tune_model(n_layers):\n",
    "  tuner = MyTuner(\n",
    "      n_layers,\n",
    "      max_trials=MAX_TRIALS,\n",
    "      executions_per_trial=EXECUTION_PER_TRIAL,\n",
    "      overwrite=True,\n",
    "      directory='my_dir',\n",
    "      project_name=f'NN_new_{n}'\n",
    "  )\n",
    "  tuner.search()\n",
    "  return tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JhJVVl4315Aj",
    "outputId": "bb38227f-72b3-49b5-eced-72c92990f5ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 01m 34s]\n",
      "default_objective: 0.040897334417414444\n",
      "\n",
      "Best default_objective So Far: 0.040897334417414444\n",
      "Total elapsed time: 00h 01m 34s\n",
      "\n",
      "Search: Running Trial #2\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "112               |16                |units_1\n",
      "48                |16                |units_2\n",
      "0.8               |0.2               |dropout\n",
      "0.0010992         |0.0001            |lr\n",
      "\n",
      "Epoch 1/16\n",
      "4920/4920 [==============================] - 6s 1ms/step - loss: 0.0246\n",
      "Epoch 2/16\n",
      "4920/4920 [==============================] - 5s 1ms/step - loss: 0.0225\n",
      "Epoch 3/16\n",
      "4920/4920 [==============================] - 5s 1ms/step - loss: 0.0225\n",
      "Epoch 4/16\n",
      "4920/4920 [==============================] - 5s 1ms/step - loss: 0.0225\n",
      "Epoch 5/16\n",
      "4920/4920 [==============================] - 6s 1ms/step - loss: 0.0224\n",
      "Epoch 6/16\n",
      "4920/4920 [==============================] - 6s 1ms/step - loss: 0.0224\n",
      "Epoch 7/16\n",
      "4920/4920 [==============================] - 5s 1ms/step - loss: 0.0223\n",
      "Epoch 8/16\n",
      "4920/4920 [==============================] - 6s 1ms/step - loss: 0.0223\n",
      "Epoch 9/16\n",
      " 193/4920 [>.............................] - ETA: 4s - loss: 0.0216"
     ]
    }
   ],
   "source": [
    "models=[]\n",
    "parameters=[]\n",
    "tuners=[]\n",
    "for n in range(2,5):\n",
    "  tuner = tune_model(n)\n",
    "  parameters.append(tuner.get_best_hyperparameters)\n",
    "  models.append(tuner.get_best_models(1))\n",
    "  tuners.append(tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('parameters.pkl','wb') as f:\n",
    "  pickle.dump(parameters,f,protocol=4)\n",
    "with open('models.pkl','wb') as f:\n",
    "  pickle.dump(parameters,f,protocol=4)\n",
    "with open('tuners.pkl','wb') as f:\n",
    "  pickle.dump(parameters,f,protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CP73ewuxc0gT"
   },
   "source": [
    "2.Use test data to get an idea of the out of sample performance of each model. Convert the standard MSE metric for out of sample performance to the “R2 out of sample” metric that was discussed in class. Compare your results to those in Gu-Kelly-Xiu and comment on the differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRGwEta1c0gU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eV0nT10sc0gU"
   },
   "source": [
    "3.Pick the model that performs the best out of sample, and interpret its output by doing the following analysis of variable importance:\n",
    "a.\tFirst, for all stock characteristics, get variable importance by setting one predictor at a time to zero and finding the decrease in out of sample R2. Show a table of the 10 most important variables according to this measure, and give an economic interpretation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sc-gagOLc0gV"
   },
   "source": [
    "b.\tSecond, get a measure of the joint importance of all our “macro predictors” (i.e., those taken from Welch and Goyal 2008), by setting them all to zero and finding the decrease in out of sample R2. Comment on how important macroeconomic variables are relative to stock characteristics in predicting returns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AX8kVzNQc0gV"
   },
   "source": [
    "c.\tRepeat the two steps above, but by using a measure of the sensitivity of predictions to each input variable, as outlined in the lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzS8KJBWc0gV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfTmKAXRc0gV"
   },
   "source": [
    "4.Fit a penalised linear model (LASSO) to the same data, using validation data to pick the best penalty (e.g., you can use the “sklearn” package in Python to do this easily). Compare its test data performance to the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yOkXRm3Oc0gW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqsoN75nc0gW"
   },
   "source": [
    "5.Suppose somebody tells you to collect 10 more micro or macro variables that can predict returns and are not in our current dataset. How would you choose those variables, based on the intuitions you have gained in this project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ni5ZNgXY1uGA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXNJN_hNc0gW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "bd_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 123,
   "position": {
    "height": "377px",
    "left": "553px",
    "right": "20px",
    "top": "104px",
    "width": "602px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
