{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d11d5662",
   "metadata": {},
   "source": [
    "This piece of code has been discarded. This is just a backup file. Although this code is good in terms of modularity it is quite cumbersome trying to retrieve the best model as the units need to be specially formatted into a list for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff04247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate(model, x, y, validation_data, **kwargs):\n",
    "  model.fit(x, y, **kwargs)\n",
    "  x_val, y_val = validation_data\n",
    "  y_pred = model.predict(x_val)\n",
    "  # Return a single float to minimize.\n",
    "  return np.mean((y_pred - y_val)**2)\n",
    "\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "def keras_code(n_layers, units, dropout_rate, learning_rate):\n",
    "    # Build model\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(shape=(105,)))\n",
    "    for i in range(n_layers):\n",
    "        model.add(layers.Dense(units=units[i], activation='relu'))\n",
    "    model.add(layers.Dropout(rate=dropout_rate))\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mse')\n",
    "    return model\n",
    "    \n",
    "    \n",
    "    \n",
    "class MyTuner(keras_tuner.RandomSearch): \n",
    "    def __init__(self, n_layers, *args, **kwargs):\n",
    "        self.n_layers = n_layers\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def run_trial(self, trial, **kwargs):\n",
    "        hp = trial.hyperparameters\n",
    "        model = keras_code(\n",
    "            n_layers=self.n_layers,\n",
    "            units=[hp.Int(f'units_{i + 1}',min_value=16,max_value=160,step=16) for i in range(self.n_layers)],\n",
    "            dropout_rate=hp.Choice('dropout_rate', values=[0.2,0.4,0.6,0.8]),\n",
    "            learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "        )\n",
    "        return fit_and_validate(\n",
    "            model, \n",
    "            X_train.values, \n",
    "            y_train.values,\n",
    "            validation_data=(X_validation.values, y_validation.values),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS\n",
    "        )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f247538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TRIALS = 8\n",
    "EXECUTION_PER_TRIAL = 3\n",
    "\n",
    "def tune_model(n_layers):\n",
    "  tuner = MyTuner(\n",
    "        n_layers,\n",
    "        max_trials=MAX_TRIALS,\n",
    "        executions_per_trial=EXECUTION_PER_TRIAL,\n",
    "        overwrite=True,\n",
    "        directory='my_dir',\n",
    "        project_name=f'NN_new_{n}'\n",
    "  )\n",
    "  tuner.search()\n",
    "  return tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535f9091",
   "metadata": {},
   "source": [
    "This is the part that makes the code impractical. What happens is that there is no way (or I have not found one) to retrieve the best model as we can do with other code foldings. TO get the best model, we need to get the bet params and then call keras_code() to create the model. However, due to the way that we manage unit optimization and due to the fact that best params returns best param for the units in each layer as an independent entry, we need to process the response. This introduces additional complexity into the code that can be avoided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae13934",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuners=[]\n",
    "parameters=[]\n",
    "models=[]\n",
    "\n",
    "for n_layers in range(2,5):\n",
    "    tuner = tune_model(n_layers)\n",
    "    tuners.append(tuner)\n",
    "    buffer_dict = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "    units = []\n",
    "    for key, value  in buffer_dict.values.items():\n",
    "         if 'units' in key:\n",
    "            units += [value]\n",
    "    best_params = {}\n",
    "    best_params['units'] = units\n",
    "    best_params['learning_rate'] = buffer_dict['learning_rate']\n",
    "    best_params['dropout_rate'] = buffer_dict['dropout_rate']\n",
    "    parameters.append(best_params)\n",
    "    model = keras_code(n_layers, **best_params)\n",
    "    models.append(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
